{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fcbabbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] - # phone classes: 41, number of utterances for train: 3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3428it [00:11, 287.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train set\n",
      "torch.Size([2116368, 1287])\n",
      "torch.Size([2116368])\n",
      "[Dataset] - # phone classes: 41, number of utterances for val: 858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "858it [00:03, 246.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] val set\n",
      "torch.Size([527790, 1287])\n",
      "torch.Size([527790])\n",
      "DEVICE: cuda:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:17<00:00, 53.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:10<00:00, 102.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/015] Train Acc: 0.618904 Loss: 1.243167 | Val Acc: 0.686127 loss: 0.996162\n",
      "saving model with acc 0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 123.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[002/015] Train Acc: 0.696164 Loss: 0.959141 | Val Acc: 0.712261 loss: 0.906985\n",
      "saving model with acc 0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[003/015] Train Acc: 0.725127 Loss: 0.859194 | Val Acc: 0.724767 loss: 0.868063\n",
      "saving model with acc 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[004/015] Train Acc: 0.744932 Loss: 0.790077 | Val Acc: 0.731674 loss: 0.848292\n",
      "saving model with acc 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005/015] Train Acc: 0.760422 Loss: 0.736621 | Val Acc: 0.738561 loss: 0.834610\n",
      "saving model with acc 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:13<00:00, 56.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 123.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[006/015] Train Acc: 0.773619 Loss: 0.691774 | Val Acc: 0.742157 loss: 0.824633\n",
      "saving model with acc 0.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:13<00:00, 55.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[007/015] Train Acc: 0.784785 Loss: 0.653115 | Val Acc: 0.745069 loss: 0.821253\n",
      "saving model with acc 0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.77it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[008/015] Train Acc: 0.795072 Loss: 0.617695 | Val Acc: 0.746782 loss: 0.824979\n",
      "saving model with acc 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[009/015] Train Acc: 0.803877 Loss: 0.586994 | Val Acc: 0.747257 loss: 0.834221\n",
      "saving model with acc 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:13<00:00, 56.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 125.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[010/015] Train Acc: 0.811756 Loss: 0.559728 | Val Acc: 0.747769 loss: 0.838709\n",
      "saving model with acc 0.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:15<00:00, 55.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[011/015] Train Acc: 0.819384 Loss: 0.534348 | Val Acc: 0.749306 loss: 0.847187\n",
      "saving model with acc 0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:13<00:00, 56.30it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 125.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[012/015] Train Acc: 0.826327 Loss: 0.511784 | Val Acc: 0.750278 loss: 0.851366\n",
      "saving model with acc 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[013/015] Train Acc: 0.832493 Loss: 0.490753 | Val Acc: 0.749575 loss: 0.861981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:13<00:00, 56.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 124.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[014/015] Train Acc: 0.838301 Loss: 0.471438 | Val Acc: 0.749639 loss: 0.875731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4134/4134 [01:14<00:00, 55.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1031/1031 [00:08<00:00, 126.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[015/015] Train Acc: 0.843604 Loss: 0.454618 | Val Acc: 0.749592 loss: 0.883738\n",
      "[Dataset] - # phone classes: 41, number of utterances for test: 1078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1078it [00:03, 282.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] test set\n",
      "torch.Size([646268, 1287])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1263/1263 [00:07<00:00, 179.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# preparing data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import math\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "def load_feat(path):\n",
    "    feat = torch.load(path)\n",
    "    return feat\n",
    "\n",
    "\n",
    "def shift(x, n):\n",
    "    if n < 0:\n",
    "        left = x[0].repeat(-n, 1)\n",
    "        right = x[:n]\n",
    "\n",
    "    elif n > 0:\n",
    "        right = x[-1].repeat(n, 1)\n",
    "        left = x[n:]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    return torch.cat((left, right), dim=0)\n",
    "\n",
    "\n",
    "def concat_feat(x, concat_n):\n",
    "    assert concat_n % 2 == 1  # n must be odd\n",
    "    if concat_n < 2:\n",
    "        return x\n",
    "    seq_len, feature_dim = x.size(0), x.size(1)\n",
    "    x = x.repeat(1, concat_n)\n",
    "    x = x.view(seq_len, concat_n, feature_dim).permute(\n",
    "        1, 0, 2)  # concat_n, seq_len, feature_dim\n",
    "    mid = (concat_n // 2)\n",
    "    for r_idx in range(1, mid+1):\n",
    "        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)\n",
    "        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)\n",
    "\n",
    "    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)\n",
    "\n",
    "\n",
    "def preprocess_data(split, feat_dir, phone_path, concat_nframes, train_ratio=0.8, train_val_seed=1337):\n",
    "    class_num = 41  # NOTE: pre-computed, should not need change\n",
    "    mode = 'train' if (split == 'train' or split == 'val') else 'test'\n",
    "\n",
    "    label_dict = {}\n",
    "    if mode != 'test':\n",
    "        phone_file = open(os.path.join(\n",
    "            phone_path, f'{mode}_labels.txt')).readlines()\n",
    "\n",
    "        for line in phone_file:\n",
    "            line = line.strip('\\n').split(' ')\n",
    "            label_dict[line[0]] = [int(p) for p in line[1:]]\n",
    "\n",
    "    if split == 'train' or split == 'val':\n",
    "        # split training and validation data\n",
    "        usage_list = open(os.path.join(\n",
    "            phone_path, 'train_split.txt')).readlines()\n",
    "        random.seed(train_val_seed)\n",
    "        random.shuffle(usage_list)\n",
    "        percent = int(len(usage_list) * train_ratio)\n",
    "        usage_list = usage_list[:percent] if split == 'train' else usage_list[percent:]\n",
    "    elif split == 'test':\n",
    "        usage_list = open(os.path.join(\n",
    "            phone_path, 'test_split.txt')).readlines()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Invalid \\'split\\' argument for dataset: PhoneDataset!')\n",
    "\n",
    "    usage_list = [line.strip('\\n') for line in usage_list]\n",
    "    print('[Dataset] - # phone classes: ' + str(class_num) +\n",
    "          ', number of utterances for ' + split + ': ' + str(len(usage_list)))\n",
    "\n",
    "    max_len = 3000000\n",
    "    X = torch.empty(max_len, 39 * concat_nframes)\n",
    "    if mode != 'test':\n",
    "        y = torch.empty(max_len, dtype=torch.long)\n",
    "\n",
    "    idx = 0\n",
    "    for i, fname in tqdm(enumerate(usage_list)):\n",
    "        feat = load_feat(os.path.join(feat_dir, mode, f'{fname}.pt'))\n",
    "        cur_len = len(feat)\n",
    "        feat = concat_feat(feat, concat_nframes)\n",
    "        if mode != 'test':\n",
    "            label = torch.LongTensor(label_dict[fname])\n",
    "\n",
    "        X[idx: idx + cur_len, :] = feat\n",
    "        if mode != 'test':\n",
    "            y[idx: idx + cur_len] = label\n",
    "\n",
    "        idx += cur_len\n",
    "\n",
    "    X = X[:idx, :]\n",
    "    if mode != 'test':\n",
    "        y = y[:idx]\n",
    "\n",
    "    print(f'[INFO] {split} set')\n",
    "    print(X.shape)\n",
    "    if mode != 'test':\n",
    "        print(y.shape)\n",
    "        return X, y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "# define datasets\n",
    "\n",
    "\n",
    "class LibriDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = X\n",
    "        if y is not None:\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# define model\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,pDrop):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        layers =[]\n",
    "        layers.append(nn.Linear(input_dim,output_dim))\n",
    "        layers.append(nn.BatchNorm1d(output_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(pDrop))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#class Classifier(nn.Module):\n",
    " #   def __init__(self, input_dim, output_dim=41, hidden_layers=1,hidden_dim=256):\n",
    "  #      super(Classifier, self).__init__()\n",
    "\n",
    "   #     self.fc = nn.Sequential(\n",
    "    #        BasicBlock(input_dim, hidden_dim,0.2),\n",
    "     #       BasicBlock()\n",
    "      #      nn.Linear(hidden_dim, output_dim)\n",
    "       # )\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    x = self.fc(x)\n",
    "     #   return x\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=41,hidden_layers=7,hidden_dim=1024,ep=1):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.ep = ep\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for i in range(hidden_layers):\n",
    "            #hidden_dim = int(hidden_dim/2)\n",
    "            layers.append(nn.Linear(input_dim,hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            p = 0.1*ep\n",
    "            if p>0.5:\n",
    "                p = 0.5\n",
    "            layers.append(nn.Dropout(p))\n",
    "            input_dim = hidden_dim\n",
    "            \n",
    "            \n",
    "        \n",
    "        self.fc = nn.Sequential(*layers,nn.Linear(hidden_dim,output_dim))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "# hyper parameters\n",
    "# data prarameters\n",
    "# the number of frames to concat with, n must be odd (total 2k+1 = n frames)\n",
    "concat_nframes = 33   #可更改的超参数，必须为奇数\n",
    "# the ratio of data used for training, the rest will be used for validation\n",
    "train_ratio = 0.8\n",
    "\n",
    "# training parameters\n",
    "seed = 0                        # random seed\n",
    "batch_size = 512                # batch size\n",
    "num_epoch = 15                 # the number of training epoch\n",
    "learning_rate = 0.0001          # learning rate\n",
    "model_path = './model.ckpt'     # the path where the checkpoint will be saved\n",
    "\n",
    "# model parameters\n",
    "# the input dim of the model, you should not change the value\n",
    "input_dim = 39 * concat_nframes\n",
    "#hidden_layers = 5               # the number of hidden layers，可更改的超参数\n",
    "hidden_dim = 1024                 # the hidden dim\n",
    "\n",
    "# prepare dataset and model\n",
    "\n",
    "# preprocess data\n",
    "train_X, train_y = preprocess_data(split='train', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio)\n",
    "val_X, val_y = preprocess_data(split='val', feat_dir='./libriphone/feat', phone_path='./libriphone', concat_nframes=concat_nframes, train_ratio=train_ratio)\n",
    "\n",
    "# get dataset\n",
    "train_set = LibriDataset(train_X, train_y)\n",
    "val_set = LibriDataset(val_X, val_y)\n",
    "\n",
    "# remove raw feature to save memory\n",
    "del train_X, train_y, val_X, val_y\n",
    "gc.collect()\n",
    "\n",
    "# get dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'DEVICE: {device}')\n",
    "\n",
    "# fix seed\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# fix random seed\n",
    "same_seeds(seed)\n",
    "\n",
    "# create model, define a loss function, and optimizer\n",
    "model = Classifier(input_dim=input_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epoch):\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train()  # set the model to training mode\n",
    "    model.ep += 1\n",
    "    model.hidden_dim = int(model.hidden_dim/2)\n",
    "    if model.hidden_layers <= 64:\n",
    "        model.hidden_layers = 64\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        features, labels = batch\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # get the index of the class with the highest probability\n",
    "        _, train_pred = torch.max(outputs, 1)\n",
    "        train_acc += (train_pred.detach() == labels.detach()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    if len(val_set) > 0:\n",
    "        model.eval()  # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(val_loader)):\n",
    "                features, labels = batch\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(features)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, val_pred = torch.max(outputs, 1)\n",
    "                # get the index of the class with the highest probability\n",
    "                val_acc += (val_pred.cpu() == labels.cpu()).sum().item()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(\n",
    "                epoch + 1, num_epoch, train_acc/len(train_set), train_loss/len(\n",
    "                    train_loader), val_acc/len(val_set), val_loss/len(val_loader)\n",
    "            ))\n",
    "\n",
    "            # if the model improves, save a checkpoint at this epoch\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print('saving model with acc {:.3f}'.format(\n",
    "                    best_acc/len(val_set)))\n",
    "    else:\n",
    "        print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "            epoch + 1, num_epoch, train_acc /\n",
    "            len(train_set), train_loss/len(train_loader)\n",
    "        ))\n",
    "if len(val_set) == 0:\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print('saving model at last epoch')\n",
    "\n",
    "del train_loader, val_loader\n",
    "gc.collect()\n",
    "\n",
    "# load data\n",
    "test_X = preprocess_data(split='test', feat_dir='./libriphone/feat',phone_path='./libriphone', concat_nframes=concat_nframes)\n",
    "test_set = LibriDataset(test_X, None)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# load model\n",
    "model = Classifier(input_dim=input_dim).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# make prediction\n",
    "test_acc = 0.0001\n",
    "test_lengths = 0\n",
    "pred = np.array([], dtype=np.int32)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        features = batch\n",
    "        features = features.to(device)\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        # get the index of the class with the highest probability\n",
    "        _, test_pred = torch.max(outputs, 1)\n",
    "        pred = np.concatenate((pred, test_pred.cpu().numpy()), axis=0)\n",
    "        \n",
    "#writing prediction csv\n",
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(pred):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00edbc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 10 15:11:18 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3080    Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    22W / 320W |      0MiB / 10018MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3080    Off  | 00000000:AF:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    21W / 320W |      0MiB / 10018MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 3080    Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    21W / 320W |      0MiB / 10018MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
